/work/dlclarge2/kumars-spino/SPINO/panoptic_label_generator/external/ms_deformable_attention/functions/deform_attn_func.py:21: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float32)
/work/dlclarge2/kumars-spino/SPINO/panoptic_label_generator/external/ms_deformable_attention/functions/deform_attn_func.py:38: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
xFormers not available
xFormers not available
[rank: 0] Seed set to 0
Traceback (most recent call last):
  File "panoptic_label_generator/semantic_fine_tuning.py", line 295, in <module>
    cli = SemanticFineTunerCLI()
  File "panoptic_label_generator/semantic_fine_tuning.py", line 282, in __init__
    super().__init__(
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/pytorch_lightning/cli.py", line 391, in __init__
    self.instantiate_classes()
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/pytorch_lightning/cli.py", line 557, in instantiate_classes
    self.config_init = self.parser.instantiate_classes(self.config)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_deprecated.py", line 147, in patched_instantiate_classes
    cfg = self._unpatched_instantiate_classes(cfg, **kwargs)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_core.py", line 1275, in instantiate_classes
    cfg[subcommand] = subparser.instantiate_classes(cfg[subcommand], instantiate_groups=instantiate_groups)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_deprecated.py", line 147, in patched_instantiate_classes
    cfg = self._unpatched_instantiate_classes(cfg, **kwargs)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_core.py", line 1269, in instantiate_classes
    component.instantiate_class(component, cfg)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_signatures.py", line 565, in group_instantiate_class
    parent[key] = instantiator_fn(group.group_class, **value)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/jsonargparse/_common.py", line 279, in __call__
    return instantiator(class_type, *args, **kwargs)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/pytorch_lightning/cli.py", line 808, in __call__
    return class_type(*args, **kwargs)
  File "panoptic_label_generator/semantic_fine_tuning.py", line 68, in __init__
    super().__init__(dinov2_vit_model=dinov2_vit_model, blocks=blocks,
  File "/work/dlclarge2/kumars-spino/SPINO/panoptic_label_generator/fine_tuning_adapter.py", line 12, in __init__
    self.encoder = ViTAdapter(
  File "/work/dlclarge2/kumars-spino/SPINO/panoptic_label_generator/models/dino_vit_adapter.py", line 50, in __init__
    self.load_state_dict(state_dict, strict=True)
  File "/work/dlclarge2/kumars-spino/minconda3/envs/spino/lib/python3.8/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ViTAdapter:
	Missing key(s) in state_dict: "blocks.0.0.norm1.weight", "blocks.0.0.norm1.bias", "blocks.0.0.attn.qkv.weight", "blocks.0.0.attn.qkv.bias", "blocks.0.0.attn.proj.weight", "blocks.0.0.attn.proj.bias", "blocks.0.0.norm2.weight", "blocks.0.0.norm2.bias", "blocks.0.0.mlp.fc1.weight", "blocks.0.0.mlp.fc1.bias", "blocks.0.0.mlp.fc2.weight", "blocks.0.0.mlp.fc2.bias", "blocks.0.1.norm1.weight", "blocks.0.1.norm1.bias", "blocks.0.1.attn.qkv.weight", "blocks.0.1.attn.qkv.bias", "blocks.0.1.attn.proj.weight", "blocks.0.1.attn.proj.bias", "blocks.0.1.norm2.weight", "blocks.0.1.norm2.bias", "blocks.0.1.mlp.fc1.weight", "blocks.0.1.mlp.fc1.bias", "blocks.0.1.mlp.fc2.weight", "blocks.0.1.mlp.fc2.bias", "blocks.0.2.norm1.weight", "blocks.0.2.norm1.bias", "blocks.0.2.attn.qkv.weight", "blocks.0.2.attn.qkv.bias", "blocks.0.2.attn.proj.weight", "blocks.0.2.attn.proj.bias", "blocks.0.2.norm2.weight", "blocks.0.2.norm2.bias", "blocks.0.2.mlp.fc1.weight", "blocks.0.2.mlp.fc1.bias", "blocks.0.2.mlp.fc2.weight", "blocks.0.2.mlp.fc2.bias", "blocks.0.3.norm1.weight", "blocks.0.3.norm1.bias", "blocks.0.3.attn.qkv.weight", "blocks.0.3.attn.qkv.bias", "blocks.0.3.attn.proj.weight", "blocks.0.3.attn.proj.bias", "blocks.0.3.norm2.weight", "blocks.0.3.norm2.bias", "blocks.0.3.mlp.fc1.weight", "blocks.0.3.mlp.fc1.bias", "blocks.0.3.mlp.fc2.weight", "blocks.0.3.mlp.fc2.bias", "blocks.0.4.norm1.weight", "blocks.0.4.norm1.bias", "blocks.0.4.attn.qkv.weight", "blocks.0.4.attn.qkv.bias", "blocks.0.4.attn.proj.weight", "blocks.0.4.attn.proj.bias", "blocks.0.4.norm2.weight", "blocks.0.4.norm2.bias", "blocks.0.4.mlp.fc1.weight", "blocks.0.4.mlp.fc1.bias", "blocks.0.4.mlp.fc2.weight", "blocks.0.4.mlp.fc2.bias", "blocks.0.5.norm1.weight", "blocks.0.5.norm1.bias", "blocks.0.5.attn.qkv.weight", "blocks.0.5.attn.qkv.bias", "blocks.0.5.attn.proj.weight", "blocks.0.5.attn.proj.bias", "blocks.0.5.norm2.weight", "blocks.0.5.norm2.bias", "blocks.0.5.mlp.fc1.weight", "blocks.0.5.mlp.fc1.bias", "blocks.0.5.mlp.fc2.weight", "blocks.0.5.mlp.fc2.bias", "blocks.0.6.norm1.weight", "blocks.0.6.norm1.bias", "blocks.0.6.attn.qkv.weight", "blocks.0.6.attn.qkv.bias", "blocks.0.6.attn.proj.weight", "blocks.0.6.attn.proj.bias", "blocks.0.6.norm2.weight", "blocks.0.6.norm2.bias", "blocks.0.6.mlp.fc1.weight", "blocks.0.6.mlp.fc1.bias", "blocks.0.6.mlp.fc2.weight", "blocks.0.6.mlp.fc2.bias", "blocks.0.7.norm1.weight", "blocks.0.7.norm1.bias", "blocks.0.7.attn.qkv.weight", "blocks.0.7.attn.qkv.bias", "blocks.0.7.attn.proj.weight", "blocks.0.7.attn.proj.bias", "blocks.0.7.norm2.weight", "blocks.0.7.norm2.bias", "blocks.0.7.mlp.fc1.weight", "blocks.0.7.mlp.fc1.bias", "blocks.0.7.mlp.fc2.weight", "blocks.0.7.mlp.fc2.bias", "blocks.0.8.norm1.weight", "blocks.0.8.norm1.bias", "blocks.0.8.attn.qkv.weight", "blocks.0.8.attn.qkv.bias", "blocks.0.8.attn.proj.weight", "blocks.0.8.attn.proj.bias", "blocks.0.8.norm2.weight", "blocks.0.8.norm2.bias", "blocks.0.8.mlp.fc1.weight", "blocks.0.8.mlp.fc1.bias", "blocks.0.8.mlp.fc2.weight", "blocks.0.8.mlp.fc2.bias", "blocks.0.9.norm1.weight", "blocks.0.9.norm1.bias", "blocks.0.9.attn.qkv.weight", "blocks.0.9.attn.qkv.bias", "blocks.0.9.attn.proj.weight", "blocks.0.9.attn.proj.bias", "blocks.0.9.norm2.weight", "blocks.0.9.norm2.bias", "blocks.0.9.mlp.fc1.weight", "blocks.0.9.mlp.fc1.bias", "blocks.0.9.mlp.fc2.weight", "blocks.0.9.mlp.fc2.bias", "blocks.0.10.norm1.weight", "blocks.0.10.norm1.bias", "blocks.0.10.attn.qkv.weight", "blocks.0.10.attn.qkv.bias", "blocks.0.10.attn.proj.weight", "blocks.0.10.attn.proj.bias", "blocks.0.10.norm2.weight", "blocks.0.10.norm2.bias", "blocks.0.10.mlp.fc1.weight", "blocks.0.10.mlp.fc1.bias", "blocks.0.10.mlp.fc2.weight", "blocks.0.10.mlp.fc2.bias", "blocks.0.11.norm1.weight", "blocks.0.11.norm1.bias", "blocks.0.11.attn.qkv.weight", "blocks.0.11.attn.qkv.bias", "blocks.0.11.attn.proj.weight", "blocks.0.11.attn.proj.bias", "blocks.0.11.norm2.weight", "blocks.0.11.norm2.bias", "blocks.0.11.mlp.fc1.weight", "blocks.0.11.mlp.fc1.bias", "blocks.0.11.mlp.fc2.weight", "blocks.0.11.mlp.fc2.bias". 
	Unexpected key(s) in state_dict: "blocks.1.norm1.weight", "blocks.1.norm1.bias", "blocks.1.attn.qkv.weight", "blocks.1.attn.qkv.bias", "blocks.1.attn.proj.weight", "blocks.1.attn.proj.bias", "blocks.1.ls1.gamma", "blocks.1.norm2.weight", "blocks.1.norm2.bias", "blocks.1.mlp.fc1.weight", "blocks.1.mlp.fc1.bias", "blocks.1.mlp.fc2.weight", "blocks.1.mlp.fc2.bias", "blocks.1.ls2.gamma", "blocks.2.norm1.weight", "blocks.2.norm1.bias", "blocks.2.attn.qkv.weight", "blocks.2.attn.qkv.bias", "blocks.2.attn.proj.weight", "blocks.2.attn.proj.bias", "blocks.2.ls1.gamma", "blocks.2.norm2.weight", "blocks.2.norm2.bias", "blocks.2.mlp.fc1.weight", "blocks.2.mlp.fc1.bias", "blocks.2.mlp.fc2.weight", "blocks.2.mlp.fc2.bias", "blocks.2.ls2.gamma", "blocks.3.norm1.weight", "blocks.3.norm1.bias", "blocks.3.attn.qkv.weight", "blocks.3.attn.qkv.bias", "blocks.3.attn.proj.weight", "blocks.3.attn.proj.bias", "blocks.3.ls1.gamma", "blocks.3.norm2.weight", "blocks.3.norm2.bias", "blocks.3.mlp.fc1.weight", "blocks.3.mlp.fc1.bias", "blocks.3.mlp.fc2.weight", "blocks.3.mlp.fc2.bias", "blocks.3.ls2.gamma", "blocks.4.norm1.weight", "blocks.4.norm1.bias", "blocks.4.attn.qkv.weight", "blocks.4.attn.qkv.bias", "blocks.4.attn.proj.weight", "blocks.4.attn.proj.bias", "blocks.4.ls1.gamma", "blocks.4.norm2.weight", "blocks.4.norm2.bias", "blocks.4.mlp.fc1.weight", "blocks.4.mlp.fc1.bias", "blocks.4.mlp.fc2.weight", "blocks.4.mlp.fc2.bias", "blocks.4.ls2.gamma", "blocks.5.norm1.weight", "blocks.5.norm1.bias", "blocks.5.attn.qkv.weight", "blocks.5.attn.qkv.bias", "blocks.5.attn.proj.weight", "blocks.5.attn.proj.bias", "blocks.5.ls1.gamma", "blocks.5.norm2.weight", "blocks.5.norm2.bias", "blocks.5.mlp.fc1.weight", "blocks.5.mlp.fc1.bias", "blocks.5.mlp.fc2.weight", "blocks.5.mlp.fc2.bias", "blocks.5.ls2.gamma", "blocks.6.norm1.weight", "blocks.6.norm1.bias", "blocks.6.attn.qkv.weight", "blocks.6.attn.qkv.bias", "blocks.6.attn.proj.weight", "blocks.6.attn.proj.bias", "blocks.6.ls1.gamma", "blocks.6.norm2.weight", "blocks.6.norm2.bias", "blocks.6.mlp.fc1.weight", "blocks.6.mlp.fc1.bias", "blocks.6.mlp.fc2.weight", "blocks.6.mlp.fc2.bias", "blocks.6.ls2.gamma", "blocks.7.norm1.weight", "blocks.7.norm1.bias", "blocks.7.attn.qkv.weight", "blocks.7.attn.qkv.bias", "blocks.7.attn.proj.weight", "blocks.7.attn.proj.bias", "blocks.7.ls1.gamma", "blocks.7.norm2.weight", "blocks.7.norm2.bias", "blocks.7.mlp.fc1.weight", "blocks.7.mlp.fc1.bias", "blocks.7.mlp.fc2.weight", "blocks.7.mlp.fc2.bias", "blocks.7.ls2.gamma", "blocks.8.norm1.weight", "blocks.8.norm1.bias", "blocks.8.attn.qkv.weight", "blocks.8.attn.qkv.bias", "blocks.8.attn.proj.weight", "blocks.8.attn.proj.bias", "blocks.8.ls1.gamma", "blocks.8.norm2.weight", "blocks.8.norm2.bias", "blocks.8.mlp.fc1.weight", "blocks.8.mlp.fc1.bias", "blocks.8.mlp.fc2.weight", "blocks.8.mlp.fc2.bias", "blocks.8.ls2.gamma", "blocks.9.norm1.weight", "blocks.9.norm1.bias", "blocks.9.attn.qkv.weight", "blocks.9.attn.qkv.bias", "blocks.9.attn.proj.weight", "blocks.9.attn.proj.bias", "blocks.9.ls1.gamma", "blocks.9.norm2.weight", "blocks.9.norm2.bias", "blocks.9.mlp.fc1.weight", "blocks.9.mlp.fc1.bias", "blocks.9.mlp.fc2.weight", "blocks.9.mlp.fc2.bias", "blocks.9.ls2.gamma", "blocks.10.norm1.weight", "blocks.10.norm1.bias", "blocks.10.attn.qkv.weight", "blocks.10.attn.qkv.bias", "blocks.10.attn.proj.weight", "blocks.10.attn.proj.bias", "blocks.10.ls1.gamma", "blocks.10.norm2.weight", "blocks.10.norm2.bias", "blocks.10.mlp.fc1.weight", "blocks.10.mlp.fc1.bias", "blocks.10.mlp.fc2.weight", "blocks.10.mlp.fc2.bias", "blocks.10.ls2.gamma", "blocks.11.norm1.weight", "blocks.11.norm1.bias", "blocks.11.attn.qkv.weight", "blocks.11.attn.qkv.bias", "blocks.11.attn.proj.weight", "blocks.11.attn.proj.bias", "blocks.11.ls1.gamma", "blocks.11.norm2.weight", "blocks.11.norm2.bias", "blocks.11.mlp.fc1.weight", "blocks.11.mlp.fc1.bias", "blocks.11.mlp.fc2.weight", "blocks.11.mlp.fc2.bias", "blocks.11.ls2.gamma", "blocks.0.norm1.weight", "blocks.0.norm1.bias", "blocks.0.attn.qkv.weight", "blocks.0.attn.qkv.bias", "blocks.0.attn.proj.weight", "blocks.0.attn.proj.bias", "blocks.0.ls1.gamma", "blocks.0.norm2.weight", "blocks.0.norm2.bias", "blocks.0.mlp.fc1.weight", "blocks.0.mlp.fc1.bias", "blocks.0.mlp.fc2.weight", "blocks.0.mlp.fc2.bias", "blocks.0.ls2.gamma". 
	size mismatch for cls_token: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 768]).
	size mismatch for pos_embed: copying a param with shape torch.Size([1, 1370, 384]) from checkpoint, the shape in current model is torch.Size([1, 257, 768]).
	size mismatch for mask_token: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 768]).
	size mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([384, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 14, 14]).
	size mismatch for patch_embed.proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
