Applying conservative parameter freezing...
  TRAINABLE (cls_token): cls_token
  TRAINABLE (pos_embed): pos_embed
  TRAINABLE (other): mask_token
  TRAINABLE (patch_embed): patch_embed.proj.weight
  TRAINABLE (patch_embed): patch_embed.proj.bias
  TRAINABLE (adapter): blocks.0.0.norm1.weight
  TRAINABLE (adapter): blocks.0.0.norm1.bias
  FROZEN (backbone): blocks.0.0.attn.qkv.weight
  FROZEN (backbone): blocks.0.0.attn.qkv.bias
  FROZEN (backbone): blocks.0.0.attn.proj.weight
  FROZEN (backbone): blocks.0.0.attn.proj.bias
  TRAINABLE (adapter): blocks.0.0.norm2.weight
  TRAINABLE (adapter): blocks.0.0.norm2.bias
  FROZEN (backbone): blocks.0.0.mlp.fc1.weight
  FROZEN (backbone): blocks.0.0.mlp.fc1.bias
  FROZEN (backbone): blocks.0.0.mlp.fc2.weight
  FROZEN (backbone): blocks.0.0.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.1.norm1.weight
  TRAINABLE (adapter): blocks.0.1.norm1.bias
  FROZEN (backbone): blocks.0.1.attn.qkv.weight
  FROZEN (backbone): blocks.0.1.attn.qkv.bias
  FROZEN (backbone): blocks.0.1.attn.proj.weight
  FROZEN (backbone): blocks.0.1.attn.proj.bias
  TRAINABLE (adapter): blocks.0.1.norm2.weight
  TRAINABLE (adapter): blocks.0.1.norm2.bias
  FROZEN (backbone): blocks.0.1.mlp.fc1.weight
  FROZEN (backbone): blocks.0.1.mlp.fc1.bias
  FROZEN (backbone): blocks.0.1.mlp.fc2.weight
  FROZEN (backbone): blocks.0.1.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.2.norm1.weight
  TRAINABLE (adapter): blocks.0.2.norm1.bias
  FROZEN (backbone): blocks.0.2.attn.qkv.weight
  FROZEN (backbone): blocks.0.2.attn.qkv.bias
  FROZEN (backbone): blocks.0.2.attn.proj.weight
  FROZEN (backbone): blocks.0.2.attn.proj.bias
  TRAINABLE (adapter): blocks.0.2.norm2.weight
  TRAINABLE (adapter): blocks.0.2.norm2.bias
  FROZEN (backbone): blocks.0.2.mlp.fc1.weight
  FROZEN (backbone): blocks.0.2.mlp.fc1.bias
  FROZEN (backbone): blocks.0.2.mlp.fc2.weight
  FROZEN (backbone): blocks.0.2.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.3.norm1.weight
  TRAINABLE (adapter): blocks.0.3.norm1.bias
  FROZEN (backbone): blocks.0.3.attn.qkv.weight
  FROZEN (backbone): blocks.0.3.attn.qkv.bias
  FROZEN (backbone): blocks.0.3.attn.proj.weight
  FROZEN (backbone): blocks.0.3.attn.proj.bias
  TRAINABLE (adapter): blocks.0.3.norm2.weight
  TRAINABLE (adapter): blocks.0.3.norm2.bias
  FROZEN (backbone): blocks.0.3.mlp.fc1.weight
  FROZEN (backbone): blocks.0.3.mlp.fc1.bias
  FROZEN (backbone): blocks.0.3.mlp.fc2.weight
  FROZEN (backbone): blocks.0.3.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.4.norm1.weight
  TRAINABLE (adapter): blocks.0.4.norm1.bias
  FROZEN (backbone): blocks.0.4.attn.qkv.weight
  FROZEN (backbone): blocks.0.4.attn.qkv.bias
  FROZEN (backbone): blocks.0.4.attn.proj.weight
  FROZEN (backbone): blocks.0.4.attn.proj.bias
  TRAINABLE (adapter): blocks.0.4.norm2.weight
  TRAINABLE (adapter): blocks.0.4.norm2.bias
  FROZEN (backbone): blocks.0.4.mlp.fc1.weight
  FROZEN (backbone): blocks.0.4.mlp.fc1.bias
  FROZEN (backbone): blocks.0.4.mlp.fc2.weight
  FROZEN (backbone): blocks.0.4.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.5.norm1.weight
  TRAINABLE (adapter): blocks.0.5.norm1.bias
  FROZEN (backbone): blocks.0.5.attn.qkv.weight
  FROZEN (backbone): blocks.0.5.attn.qkv.bias
  FROZEN (backbone): blocks.0.5.attn.proj.weight
  FROZEN (backbone): blocks.0.5.attn.proj.bias
  TRAINABLE (adapter): blocks.0.5.norm2.weight
  TRAINABLE (adapter): blocks.0.5.norm2.bias
  FROZEN (backbone): blocks.0.5.mlp.fc1.weight
  FROZEN (backbone): blocks.0.5.mlp.fc1.bias
  FROZEN (backbone): blocks.0.5.mlp.fc2.weight
  FROZEN (backbone): blocks.0.5.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.6.norm1.weight
  TRAINABLE (adapter): blocks.0.6.norm1.bias
  FROZEN (backbone): blocks.0.6.attn.qkv.weight
  FROZEN (backbone): blocks.0.6.attn.qkv.bias
  FROZEN (backbone): blocks.0.6.attn.proj.weight
  FROZEN (backbone): blocks.0.6.attn.proj.bias
  TRAINABLE (adapter): blocks.0.6.norm2.weight
  TRAINABLE (adapter): blocks.0.6.norm2.bias
  FROZEN (backbone): blocks.0.6.mlp.fc1.weight
  FROZEN (backbone): blocks.0.6.mlp.fc1.bias
  FROZEN (backbone): blocks.0.6.mlp.fc2.weight
  FROZEN (backbone): blocks.0.6.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.7.norm1.weight
  TRAINABLE (adapter): blocks.0.7.norm1.bias
  FROZEN (backbone): blocks.0.7.attn.qkv.weight
  FROZEN (backbone): blocks.0.7.attn.qkv.bias
  FROZEN (backbone): blocks.0.7.attn.proj.weight
  FROZEN (backbone): blocks.0.7.attn.proj.bias
  TRAINABLE (adapter): blocks.0.7.norm2.weight
  TRAINABLE (adapter): blocks.0.7.norm2.bias
  FROZEN (backbone): blocks.0.7.mlp.fc1.weight
  FROZEN (backbone): blocks.0.7.mlp.fc1.bias
  FROZEN (backbone): blocks.0.7.mlp.fc2.weight
  FROZEN (backbone): blocks.0.7.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.8.norm1.weight
  TRAINABLE (adapter): blocks.0.8.norm1.bias
  FROZEN (backbone): blocks.0.8.attn.qkv.weight
  FROZEN (backbone): blocks.0.8.attn.qkv.bias
  FROZEN (backbone): blocks.0.8.attn.proj.weight
  FROZEN (backbone): blocks.0.8.attn.proj.bias
  TRAINABLE (adapter): blocks.0.8.norm2.weight
  TRAINABLE (adapter): blocks.0.8.norm2.bias
  FROZEN (backbone): blocks.0.8.mlp.fc1.weight
  FROZEN (backbone): blocks.0.8.mlp.fc1.bias
  FROZEN (backbone): blocks.0.8.mlp.fc2.weight
  FROZEN (backbone): blocks.0.8.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.9.norm1.weight
  TRAINABLE (adapter): blocks.0.9.norm1.bias
  FROZEN (backbone): blocks.0.9.attn.qkv.weight
  FROZEN (backbone): blocks.0.9.attn.qkv.bias
  FROZEN (backbone): blocks.0.9.attn.proj.weight
  FROZEN (backbone): blocks.0.9.attn.proj.bias
  TRAINABLE (adapter): blocks.0.9.norm2.weight
  TRAINABLE (adapter): blocks.0.9.norm2.bias
  FROZEN (backbone): blocks.0.9.mlp.fc1.weight
  FROZEN (backbone): blocks.0.9.mlp.fc1.bias
  FROZEN (backbone): blocks.0.9.mlp.fc2.weight
  FROZEN (backbone): blocks.0.9.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.10.norm1.weight
  TRAINABLE (adapter): blocks.0.10.norm1.bias
  FROZEN (backbone): blocks.0.10.attn.qkv.weight
  FROZEN (backbone): blocks.0.10.attn.qkv.bias
  FROZEN (backbone): blocks.0.10.attn.proj.weight
  FROZEN (backbone): blocks.0.10.attn.proj.bias
  TRAINABLE (adapter): blocks.0.10.norm2.weight
  TRAINABLE (adapter): blocks.0.10.norm2.bias
  FROZEN (backbone): blocks.0.10.mlp.fc1.weight
  FROZEN (backbone): blocks.0.10.mlp.fc1.bias
  FROZEN (backbone): blocks.0.10.mlp.fc2.weight
  FROZEN (backbone): blocks.0.10.mlp.fc2.bias
  TRAINABLE (adapter): blocks.0.11.norm1.weight
  TRAINABLE (adapter): blocks.0.11.norm1.bias
  FROZEN (backbone): blocks.0.11.attn.qkv.weight
  FROZEN (backbone): blocks.0.11.attn.qkv.bias
  FROZEN (backbone): blocks.0.11.attn.proj.weight
  FROZEN (backbone): blocks.0.11.attn.proj.bias
  TRAINABLE (adapter): blocks.0.11.norm2.weight
  TRAINABLE (adapter): blocks.0.11.norm2.bias
  FROZEN (backbone): blocks.0.11.mlp.fc1.weight
  FROZEN (backbone): blocks.0.11.mlp.fc1.bias
  FROZEN (backbone): blocks.0.11.mlp.fc2.weight
  FROZEN (backbone): blocks.0.11.mlp.fc2.bias
  TRAINABLE (final_norm): norm.weight
  TRAINABLE (final_norm): norm.bias
=== Parameter Analysis ===

Trainable parameters:
  ✓ cls_token: 384 params
  ✓ pos_embed: 526080 params
  ✓ level_embed: 1152 params
  ✓ patch_embed.proj.weight: 225792 params
  ✓ patch_embed.proj.bias: 384 params
  ✓ blocks.0.0.norm1.weight: 384 params
  ✓ blocks.0.0.norm1.bias: 384 params
  ✓ blocks.0.0.norm2.weight: 384 params
  ✓ blocks.0.0.norm2.bias: 384 params
  ✓ blocks.0.1.norm1.weight: 384 params
  ✓ blocks.0.1.norm1.bias: 384 params
  ✓ blocks.0.1.norm2.weight: 384 params
  ✓ blocks.0.1.norm2.bias: 384 params
  ✓ blocks.0.2.norm1.weight: 384 params
  ✓ blocks.0.2.norm1.bias: 384 params
  ✓ blocks.0.2.norm2.weight: 384 params
  ✓ blocks.0.2.norm2.bias: 384 params
  ✓ blocks.0.3.norm1.weight: 384 params
  ✓ blocks.0.3.norm1.bias: 384 params
  ✓ blocks.0.3.norm2.weight: 384 params
  ✓ blocks.0.3.norm2.bias: 384 params
  ✓ blocks.0.4.norm1.weight: 384 params
  ✓ blocks.0.4.norm1.bias: 384 params
  ✓ blocks.0.4.norm2.weight: 384 params
  ✓ blocks.0.4.norm2.bias: 384 params
  ✓ blocks.0.5.norm1.weight: 384 params
  ✓ blocks.0.5.norm1.bias: 384 params
  ✓ blocks.0.5.norm2.weight: 384 params
  ✓ blocks.0.5.norm2.bias: 384 params
  ✓ blocks.0.6.norm1.weight: 384 params
  ✓ blocks.0.6.norm1.bias: 384 params
  ✓ blocks.0.6.norm2.weight: 384 params
  ✓ blocks.0.6.norm2.bias: 384 params
  ✓ blocks.0.7.norm1.weight: 384 params
  ✓ blocks.0.7.norm1.bias: 384 params
  ✓ blocks.0.7.norm2.weight: 384 params
  ✓ blocks.0.7.norm2.bias: 384 params
  ✓ blocks.0.8.norm1.weight: 384 params
  ✓ blocks.0.8.norm1.bias: 384 params
  ✓ blocks.0.8.norm2.weight: 384 params
  ✓ blocks.0.8.norm2.bias: 384 params
  ✓ blocks.0.9.norm1.weight: 384 params
  ✓ blocks.0.9.norm1.bias: 384 params
  ✓ blocks.0.9.norm2.weight: 384 params
  ✓ blocks.0.9.norm2.bias: 384 params
  ✓ blocks.0.10.norm1.weight: 384 params
  ✓ blocks.0.10.norm1.bias: 384 params
  ✓ blocks.0.10.norm2.weight: 384 params
  ✓ blocks.0.10.norm2.bias: 384 params
  ✓ blocks.0.11.norm1.weight: 384 params
  ✓ blocks.0.11.norm1.bias: 384 params
  ✓ blocks.0.11.norm2.weight: 384 params
  ✓ blocks.0.11.norm2.bias: 384 params
  ✓ norm.weight: 384 params
  ✓ norm.bias: 384 params
  ✓ spm.stem.0.weight: 1728 params
  ✓ spm.stem.1.weight: 64 params
  ✓ spm.stem.1.bias: 64 params
  ✓ spm.stem.3.weight: 36864 params
  ✓ spm.stem.4.weight: 64 params
  ✓ spm.stem.4.bias: 64 params
  ✓ spm.stem.6.weight: 36864 params
  ✓ spm.stem.7.weight: 64 params
  ✓ spm.stem.7.bias: 64 params
  ✓ spm.conv2.0.weight: 73728 params
  ✓ spm.conv2.1.weight: 128 params
  ✓ spm.conv2.1.bias: 128 params
  ✓ spm.conv3.0.weight: 294912 params
  ✓ spm.conv3.1.weight: 256 params
  ✓ spm.conv3.1.bias: 256 params
  ✓ spm.conv4.0.weight: 589824 params
  ✓ spm.conv4.1.weight: 256 params
  ✓ spm.conv4.1.bias: 256 params
  ✓ spm.fc1.weight: 24576 params
  ✓ spm.fc1.bias: 384 params
  ✓ spm.fc2.weight: 49152 params
  ✓ spm.fc2.bias: 384 params
  ✓ spm.fc3.weight: 98304 params
  ✓ spm.fc3.bias: 384 params
  ✓ spm.fc4.weight: 98304 params
  ✓ spm.fc4.bias: 384 params
  ✓ interactions.0.injector.gamma: 384 params
  ✓ interactions.0.injector.query_norm.weight: 384 params
  ✓ interactions.0.injector.query_norm.bias: 384 params
  ✓ interactions.0.injector.feat_norm.weight: 384 params
  ✓ interactions.0.injector.feat_norm.bias: 384 params
  ✓ interactions.0.injector.attn.sampling_offsets.weight: 55296 params
  ✓ interactions.0.injector.attn.sampling_offsets.bias: 144 params
  ✓ interactions.0.injector.attn.attention_weights.weight: 27648 params
  ✓ interactions.0.injector.attn.attention_weights.bias: 72 params
  ✓ interactions.0.injector.attn.value_proj.weight: 147456 params
  ✓ interactions.0.injector.attn.value_proj.bias: 384 params
  ✓ interactions.0.injector.attn.output_proj.weight: 147456 params
  ✓ interactions.0.injector.attn.output_proj.bias: 384 params
  ✓ interactions.0.extractor.query_norm.weight: 384 params
  ✓ interactions.0.extractor.query_norm.bias: 384 params
  ✓ interactions.0.extractor.feat_norm.weight: 384 params
  ✓ interactions.0.extractor.feat_norm.bias: 384 params
  ✓ interactions.0.extractor.attn.sampling_offsets.weight: 18432 params
  ✓ interactions.0.extractor.attn.sampling_offsets.bias: 48 params
  ✓ interactions.0.extractor.attn.attention_weights.weight: 9216 params
  ✓ interactions.0.extractor.attn.attention_weights.bias: 24 params
  ✓ interactions.0.extractor.attn.value_proj.weight: 147456 params
  ✓ interactions.0.extractor.attn.value_proj.bias: 384 params
  ✓ interactions.0.extractor.attn.output_proj.weight: 147456 params
  ✓ interactions.0.extractor.attn.output_proj.bias: 384 params
  ✓ interactions.0.extractor.ffn.fc1.weight: 36864 params
  ✓ interactions.0.extractor.ffn.fc1.bias: 96 params
  ✓ interactions.0.extractor.ffn.dwconv.dwconv.weight: 864 params
  ✓ interactions.0.extractor.ffn.dwconv.dwconv.bias: 96 params
  ✓ interactions.0.extractor.ffn.fc2.weight: 36864 params
  ✓ interactions.0.extractor.ffn.fc2.bias: 384 params
  ✓ interactions.0.extractor.ffn_norm.weight: 384 params
  ✓ interactions.0.extractor.ffn_norm.bias: 384 params
  ✓ interactions.1.injector.gamma: 384 params
  ✓ interactions.1.injector.query_norm.weight: 384 params
  ✓ interactions.1.injector.query_norm.bias: 384 params
  ✓ interactions.1.injector.feat_norm.weight: 384 params
  ✓ interactions.1.injector.feat_norm.bias: 384 params
  ✓ interactions.1.injector.attn.sampling_offsets.weight: 55296 params
  ✓ interactions.1.injector.attn.sampling_offsets.bias: 144 params
  ✓ interactions.1.injector.attn.attention_weights.weight: 27648 params
  ✓ interactions.1.injector.attn.attention_weights.bias: 72 params
  ✓ interactions.1.injector.attn.value_proj.weight: 147456 params
  ✓ interactions.1.injector.attn.value_proj.bias: 384 params
  ✓ interactions.1.injector.attn.output_proj.weight: 147456 params
  ✓ interactions.1.injector.attn.output_proj.bias: 384 params
  ✓ interactions.1.extractor.query_norm.weight: 384 params
  ✓ interactions.1.extractor.query_norm.bias: 384 params
  ✓ interactions.1.extractor.feat_norm.weight: 384 params
  ✓ interactions.1.extractor.feat_norm.bias: 384 params
  ✓ interactions.1.extractor.attn.sampling_offsets.weight: 18432 params
  ✓ interactions.1.extractor.attn.sampling_offsets.bias: 48 params
  ✓ interactions.1.extractor.attn.attention_weights.weight: 9216 params
  ✓ interactions.1.extractor.attn.attention_weights.bias: 24 params
  ✓ interactions.1.extractor.attn.value_proj.weight: 147456 params
  ✓ interactions.1.extractor.attn.value_proj.bias: 384 params
  ✓ interactions.1.extractor.attn.output_proj.weight: 147456 params
  ✓ interactions.1.extractor.attn.output_proj.bias: 384 params
  ✓ interactions.1.extractor.ffn.fc1.weight: 36864 params
  ✓ interactions.1.extractor.ffn.fc1.bias: 96 params
  ✓ interactions.1.extractor.ffn.dwconv.dwconv.weight: 864 params
  ✓ interactions.1.extractor.ffn.dwconv.dwconv.bias: 96 params
  ✓ interactions.1.extractor.ffn.fc2.weight: 36864 params
  ✓ interactions.1.extractor.ffn.fc2.bias: 384 params
  ✓ interactions.1.extractor.ffn_norm.weight: 384 params
  ✓ interactions.1.extractor.ffn_norm.bias: 384 params
  ✓ interactions.2.injector.gamma: 384 params
  ✓ interactions.2.injector.query_norm.weight: 384 params
  ✓ interactions.2.injector.query_norm.bias: 384 params
  ✓ interactions.2.injector.feat_norm.weight: 384 params
  ✓ interactions.2.injector.feat_norm.bias: 384 params
  ✓ interactions.2.injector.attn.sampling_offsets.weight: 55296 params
  ✓ interactions.2.injector.attn.sampling_offsets.bias: 144 params
  ✓ interactions.2.injector.attn.attention_weights.weight: 27648 params
  ✓ interactions.2.injector.attn.attention_weights.bias: 72 params
  ✓ interactions.2.injector.attn.value_proj.weight: 147456 params
  ✓ interactions.2.injector.attn.value_proj.bias: 384 params
  ✓ interactions.2.injector.attn.output_proj.weight: 147456 params
  ✓ interactions.2.injector.attn.output_proj.bias: 384 params
  ✓ interactions.2.extractor.query_norm.weight: 384 params
  ✓ interactions.2.extractor.query_norm.bias: 384 params
  ✓ interactions.2.extractor.feat_norm.weight: 384 params
  ✓ interactions.2.extractor.feat_norm.bias: 384 params
  ✓ interactions.2.extractor.attn.sampling_offsets.weight: 18432 params
  ✓ interactions.2.extractor.attn.sampling_offsets.bias: 48 params
  ✓ interactions.2.extractor.attn.attention_weights.weight: 9216 params
  ✓ interactions.2.extractor.attn.attention_weights.bias: 24 params
  ✓ interactions.2.extractor.attn.value_proj.weight: 147456 params
  ✓ interactions.2.extractor.attn.value_proj.bias: 384 params
  ✓ interactions.2.extractor.attn.output_proj.weight: 147456 params
  ✓ interactions.2.extractor.attn.output_proj.bias: 384 params
  ✓ interactions.2.extractor.ffn.fc1.weight: 36864 params
  ✓ interactions.2.extractor.ffn.fc1.bias: 96 params
  ✓ interactions.2.extractor.ffn.dwconv.dwconv.weight: 864 params
  ✓ interactions.2.extractor.ffn.dwconv.dwconv.bias: 96 params
  ✓ interactions.2.extractor.ffn.fc2.weight: 36864 params
  ✓ interactions.2.extractor.ffn.fc2.bias: 384 params
  ✓ interactions.2.extractor.ffn_norm.weight: 384 params
  ✓ interactions.2.extractor.ffn_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.0.query_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.0.query_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.0.feat_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.0.feat_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.0.attn.sampling_offsets.weight: 18432 params
  ✓ interactions.2.extra_extractors.0.attn.sampling_offsets.bias: 48 params
  ✓ interactions.2.extra_extractors.0.attn.attention_weights.weight: 9216 params
  ✓ interactions.2.extra_extractors.0.attn.attention_weights.bias: 24 params
  ✓ interactions.2.extra_extractors.0.attn.value_proj.weight: 147456 params
  ✓ interactions.2.extra_extractors.0.attn.value_proj.bias: 384 params
  ✓ interactions.2.extra_extractors.0.attn.output_proj.weight: 147456 params
  ✓ interactions.2.extra_extractors.0.attn.output_proj.bias: 384 params
  ✓ interactions.2.extra_extractors.0.ffn.fc1.weight: 36864 params
  ✓ interactions.2.extra_extractors.0.ffn.fc1.bias: 96 params
  ✓ interactions.2.extra_extractors.0.ffn.dwconv.dwconv.weight: 864 params
  ✓ interactions.2.extra_extractors.0.ffn.dwconv.dwconv.bias: 96 params
  ✓ interactions.2.extra_extractors.0.ffn.fc2.weight: 36864 params
  ✓ interactions.2.extra_extractors.0.ffn.fc2.bias: 384 params
  ✓ interactions.2.extra_extractors.0.ffn_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.0.ffn_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.1.query_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.1.query_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.1.feat_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.1.feat_norm.bias: 384 params
  ✓ interactions.2.extra_extractors.1.attn.sampling_offsets.weight: 18432 params
  ✓ interactions.2.extra_extractors.1.attn.sampling_offsets.bias: 48 params
  ✓ interactions.2.extra_extractors.1.attn.attention_weights.weight: 9216 params
  ✓ interactions.2.extra_extractors.1.attn.attention_weights.bias: 24 params
  ✓ interactions.2.extra_extractors.1.attn.value_proj.weight: 147456 params
  ✓ interactions.2.extra_extractors.1.attn.value_proj.bias: 384 params
  ✓ interactions.2.extra_extractors.1.attn.output_proj.weight: 147456 params
  ✓ interactions.2.extra_extractors.1.attn.output_proj.bias: 384 params
  ✓ interactions.2.extra_extractors.1.ffn.fc1.weight: 36864 params
  ✓ interactions.2.extra_extractors.1.ffn.fc1.bias: 96 params
  ✓ interactions.2.extra_extractors.1.ffn.dwconv.dwconv.weight: 864 params
  ✓ interactions.2.extra_extractors.1.ffn.dwconv.dwconv.bias: 96 params
  ✓ interactions.2.extra_extractors.1.ffn.fc2.weight: 36864 params
  ✓ interactions.2.extra_extractors.1.ffn.fc2.bias: 384 params
  ✓ interactions.2.extra_extractors.1.ffn_norm.weight: 384 params
  ✓ interactions.2.extra_extractors.1.ffn_norm.bias: 384 params
  ✓ up.weight: 589824 params
  ✓ up.bias: 384 params
  ✓ norm1.weight: 384 params
  ✓ norm1.bias: 384 params
  ✓ norm2.weight: 384 params
  ✓ norm2.bias: 384 params
  ✓ norm3.weight: 384 params
  ✓ norm3.bias: 384 params
  ✓ norm4.weight: 384 params
  ✓ norm4.bias: 384 params

Frozen parameters: 21275136
Total DINOv2 blocks: 1
Trainable blocks: [0]

=== Summary ===
Total parameters: 27,095,504
Trainable parameters: 5,820,368 (21.5%)
Frozen parameters: 21,275,136 (78.5%)
Adapter parameters: 5,067,728
Backbone parameters: 22,027,776

=== Forward Pass Test ===
/work/dlclarge2/masoodw-spino100/miniconda3/envs/spino/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
✗ Forward pass failed: Not implemented on the CPU